{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Chunking\n",
    "Unlike fixed length chunking which we saw in simple RAG, Semantic chunking divides the content based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import ollama\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chnologies serve the varied needs of developing nations. \n",
      " \n",
      "At Intripid, I spearheaded the development of an AI travel product that not only captivated over 10,000 \n",
      "users but also demonstrated my ability to align innovative technology with market demands. By designing \n",
      "a multi-agent Retrieval Augmented Generation model, I significantly enhanced the efficiency of itinerary \n",
      "generation by 60%. This experience taught me the importance of understanding user needs and \n",
      "collaborating with diverse teams, skills that I believe are crucial for driving successful AI initiatives at the \n",
      "World Bank. My role involved engaging with venture capital firms and academic institutions, allowing me \n",
      "to navigate complex stakeholder interests effectively while securing $150K in investment capital to propel \n",
      "our vision forward. This blend of technical expertise and strategic communication positions me well to \n",
      "support the World Bank's mission of leveraging AI for transformative development in emerging markets. \n",
      " \n",
      "I am genuinely excited about the opportunity to contribute to the World Bank's mission as an Artificial \n",
      "Intelligence Specialist. I am eager to leverage my expertise in AI and digital transformation to drive \n",
      "impactful initiatives that address the unique challenges faced by emerging markets. Thank you for \n",
      "considering my application. \n",
      " \n",
      " \n",
      "Regards, \n",
      "Mahikshit Kurapati\n"
     ]
    }
   ],
   "source": [
    "#Extracting text from pdf files\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    my_pdf = pymupdf.open(pdf_path)\n",
    "    all_text = ''\n",
    "\n",
    "    for page in my_pdf:\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    return all_text.strip()\n",
    "\n",
    "pdf_path = '/Users/mahikshitk/Documents/cover letters/Mahikshit_Kurapati_cover_letter_world_bank.pdf'\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"nomic-embed-text:latest\"):\n",
    "\n",
    "    response = ollama.embed(model = model, input=text)\n",
    "    return np.array(response.embeddings[0])\n",
    "\n",
    "sentences = extracted_text.split(\". \")\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "\n",
    "    return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "similarities = [cosine_similarity(embeddings[i], embeddings[i+1]) for i in range(len(embeddings)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoints: [0, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    if method == \"percentile\":\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "\n",
    "    elif method == \"standard_deviation\":\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        threshold_value = mean - (std_dev * threshold)\n",
    "\n",
    "    elif method == \"interquartile\":\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        threshold_value = q1 - 1.5 * (q3-q1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "    \n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n",
    "\n",
    "breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=90)\n",
    "print(\"Breakpoints:\", breakpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic chunks: 10\n",
      "Mahikshit Kurapati \n",
      "+15717740349 | mahikshitkurapati@gmail.com | linkedin.com/in/mahikshitkurapati \n",
      " \n",
      " \n",
      " \n",
      "Dear team at World Bank, \n",
      " \n",
      "As a Machine Learning Engineer with a proven track record of transforming complex AI concepts into \n",
      "actionable solutions, I thrive in navigating diverse stakeholder landscapes.\n"
     ]
    }
   ],
   "source": [
    "from cgitb import text\n",
    "\n",
    "\n",
    "def split_into_chunks(sentences, breakpoints):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for bp in breakpoints:\n",
    "        chunks.append(\". \".join(sentences[start:bp + 1]) + \".\")\n",
    "        start = bp + 1\n",
    "\n",
    "    chunks.append(\". \".join(sentences[start:]))\n",
    "    return chunks\n",
    "\n",
    "text_chunks = split_into_chunks(sentences, breakpoints)\n",
    "\n",
    "print(f\"Number of semantic chunks: {len(text_chunks)}\")\n",
    "\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text_chunks):\n",
    "\n",
    "    return [get_embedding(chunk) for chunk in text_chunks]\n",
    "\n",
    "chunk_embeddings = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, chunk_embeddings, k=5):\n",
    "\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "\n",
    "\n",
    "    return [(text_chunks[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which all places did Mahikshit work at?\n",
      "Context 1:\n",
      "('\\n \\n \\nRegards, \\nMahikshit Kurapati', np.float64(0.6828681500401532))\n",
      "========================================\n",
      "Context 2:\n",
      "('Mahikshit Kurapati \\n+15717740349 | mahikshitkurapati@gmail.com | linkedin.com/in/mahikshitkurapati \\n \\n \\n \\nDear team at World Bank, \\n \\nAs a Machine Learning Engineer with a proven track record of transforming complex AI concepts into \\nactionable solutions, I thrive in navigating diverse stakeholder landscapes.', np.float64(0.6551582148822156))\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "query = \"Which all places did Mahikshit work at?\"\n",
    "\n",
    "top_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=2)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i+1}:\\n{chunk}\\n{'='*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
